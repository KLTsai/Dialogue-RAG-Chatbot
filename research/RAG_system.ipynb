{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1156e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c958c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27da631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataEmbeddingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_name: str\n",
    "    text_column: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class VectorStorageConfig:\n",
    "    data_path: Path\n",
    "    embedding_dim: int\n",
    "\n",
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    # Gemini API è¨­å®š\n",
    "    gemini_api_key: str\n",
    "    gemini_model: str\n",
    "    gemini_temperature: float\n",
    "    gemini_max_output_tokens: int\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.gemini_api_key:\n",
    "            raise ValueError(\"è«‹è¨­å®š GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–åœ¨é…ç½®ä¸­æä¾› API key\")\n",
    "        \n",
    "@dataclass\n",
    "class RAGSystemConfig:\n",
    "    max_retrieval_docs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c86fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialogue_rag_chatbot.constants import *\n",
    "from dialogue_rag_chatbot.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_embedding_config(self) -> DataEmbeddingConfig:\n",
    "        config = self.config.data_embedding\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        data_embedding_config = DataEmbeddingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_name = config.model_name,\n",
    "            text_column= config.text_column\n",
    "        )\n",
    "\n",
    "        return data_embedding_config\n",
    "\n",
    "    def get_vector_storage_config(self)-> VectorStorageConfig:\n",
    "\n",
    "        config = self.config.vector_storage\n",
    "        vector_storage_config = VectorStorageConfig(\n",
    "            data_path=config.data_path,\n",
    "            embedding_dim=config.embedding_dim\n",
    "        )\n",
    "\n",
    "        return vector_storage_config\n",
    "    \n",
    "    def get_gemini_config(self)-> GeminiConfig:\n",
    "        config = self.config.gemini\n",
    "        gemini_config = GeminiConfig(\n",
    "            gemini_api_key=os.getenv(\"GEMINI_API_KEY\", \"\"),\n",
    "            gemini_model=config.gemini_model,\n",
    "            gemini_temperature= config.gemini_temperature,\n",
    "            gemini_max_output_tokens=config.gemini_max_output_tokens\n",
    "        )\n",
    "\n",
    "        return gemini_config\n",
    "    \n",
    "    def get_rag_sys_config(self)-> RAGSystemConfig:\n",
    "        config = self.config.RAG_system\n",
    "        rag_sys_config = RAGSystemConfig(\n",
    "            max_retrieval_docs = config.max_retrieval_docs\n",
    "        )\n",
    "\n",
    "        return rag_sys_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kun\\miniconda3\\envs\\legalrag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dialogue_rag_chatbot.logging import logger\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from enum import Enum\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01439bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveDecision(str, Enum):\n",
    "    \"\"\"æª¢ç´¢æ±ºç­–æšèˆ‰\"\"\"\n",
    "    YES = \"yes\"\n",
    "    NO = \"no\" \n",
    "    CONTINUE = \"continue\"\n",
    "\n",
    "class IsREL(str, Enum):\n",
    "    \"\"\"ç›¸é—œæ€§åˆ¤æ–·æšèˆ‰\"\"\"\n",
    "    RELEVANT = \"relevant\"\n",
    "    IRRELEVANT = \"irrelevant\"\n",
    "\n",
    "class IsSUP(str, Enum):\n",
    "    \"\"\"æ”¯æ’æ€§åˆ¤æ–·æšèˆ‰\"\"\"\n",
    "    FULLY_SUPPORTED = \"fully supported\"\n",
    "    PARTIALLY_SUPPORTED = \"partially supported\"\n",
    "    NO_SUPPORT = \"no support\"\n",
    "\n",
    "class IsUSE(int, Enum):\n",
    "    \"\"\"æœ‰ç”¨æ€§è©•åˆ†æšèˆ‰\"\"\"\n",
    "    VERY_USEFUL = 5\n",
    "    USEFUL = 4\n",
    "    MODERATELY_USEFUL = 3\n",
    "    LESS_USEFUL = 2\n",
    "    NOT_USEFUL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbfffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Granite embedding model using SentenceTransformers\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataEmbeddingConfig):\n",
    "        self.model_name = config.model_name\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            logger.info(f\"Granite embedding model '{self.model_name}' loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {self.model_name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def encode(self, texts):\n",
    "        \"\"\"Encode texts into embeddings\"\"\"\n",
    "        try:\n",
    "            # Granite embedding models return 768-dimensional vectors\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "            logger.info(f\"Encoded {len(texts)} texts into embeddings of shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encoding texts: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class VectorStorage:\n",
    "    \"\"\"Store and retrieve document embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VectorStorageConfig):\n",
    "        self.config = config\n",
    "        self.embedding_dim = self.config.embedding_dim\n",
    "        self.embeddings = []\n",
    "        self.documents = []\n",
    "        logger.info(f\"VectorStore initialized with embedding_dim={self.embedding_dim}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the store\"\"\"\n",
    "        if embeddings.shape[1] != self.embedding_dim:\n",
    "            raise ValueError(f\"Embedding dimension mismatch: expected {self.embedding_dim}, got {embeddings.shape[1]}\")\n",
    "        \n",
    "        self.embeddings.extend(embeddings.tolist())\n",
    "        self.documents.extend(documents)\n",
    "        logger.info(f\"Added {len(documents)} documents to vector store\")\n",
    "    \n",
    "    def similarity_search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        \"\"\"Perform similarity search\"\"\"\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = query_embedding.flatten()\n",
    "        \n",
    "        embeddings_matrix = np.array(self.embeddings)\n",
    "        similarities = np.dot(embeddings_matrix, query_embedding.T) / (\n",
    "            np.linalg.norm(embeddings_matrix, axis=1) * np.linalg.norm(query_embedding) + 1e-10\n",
    "        )\n",
    "        \n",
    "        # top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        top_indices = np.argpartition(-similarities, top_k)[:top_k]\n",
    "        top_indices = top_indices[np.argsort(-similarities[top_indices])]\n",
    "\n",
    "        results = [(self.documents[idx], float(similarities[idx])) for idx in top_indices]\n",
    "        logger.info(f\"Retrieved {len(results)} documents from vector store\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19369ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiClient:\n",
    "    \"\"\"Gemini API å®¢æˆ¶ç«¯å°è£\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GeminiConfig):\n",
    "        self.config = config\n",
    "        self.client = genai.Client(api_key=config.gemini_api_key)\n",
    "        logger.info(f\"Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {config.gemini_model}\")\n",
    "    \n",
    "    def generate_content(self, prompt: str, context: str = \"\") -> str:\n",
    "        \"\"\"ç”Ÿæˆæ–‡æœ¬å…§å®¹\"\"\"\n",
    "        try:\n",
    "            full_prompt = f\"{context}\\n\\n{prompt}\" if context else prompt\n",
    "\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.config.gemini_model,\n",
    "                contents=full_prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=self.config.gemini_temperature,\n",
    "                    max_output_tokens=self.config.gemini_max_output_tokens\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"ç”¢ç”Ÿå…§å®¹===>: {response.candidates[0].content.parts[0].text}\")\n",
    "            \n",
    "            if response and hasattr(response, 'text') and response.text:\n",
    "                return response.text.strip()\n",
    "            else:\n",
    "                # å¦‚æœå› ç‚ºå®‰å…¨è¨­å®šç­‰åŸå› æ²’æœ‰æ–‡å­—è¿”å›ï¼Œå‰‡è¿”å›ç©ºå­—ä¸²\n",
    "                logger.warning(f\"Gemini API did not return text. Full response: {response}\")\n",
    "                return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini API å‘¼å«å¤±æ•—: {str(e)}\")\n",
    "            return \"æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚\"\n",
    "    \n",
    "    def predict_retrieve(self, query: str, previous_generation: str = \"\") -> RetrieveDecision:\n",
    "        \"\"\"\n",
    "        M predicts Retrieve given (x, y_{t-1})\n",
    "        åˆ¤æ–·æ˜¯å¦éœ€è¦æª¢ç´¢å°è©±è³‡æ–™ä¾†å›ç­”å•é¡Œ\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    è«‹åˆ¤æ–·ä»¥ä¸‹ä½¿ç”¨è€…æŸ¥è©¢æ˜¯å¦éœ€è¦æª¢ç´¢å°è©±è³‡æ–™åº«ä¾†å›ç­”ï¼š\n",
    "\n",
    "                    ç›®å‰ä½¿ç”¨è€…æŸ¥è©¢: {query}\n",
    "                    ä¹‹å‰çš„å°è©±å…§å®¹: {previous_generation if previous_generation else \"ç„¡\"}\n",
    "\n",
    "                    åˆ¤æ–·æ¨™æº–ï¼š\n",
    "                    - yes: æŸ¥è©¢éœ€è¦å…·é«”çš„å°è©±å…§å®¹æˆ–å°è©±å ´æ™¯ä¾†å›ç­”ã€‚å¦‚æœæœ‰ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œè«‹å‹™å¿…ä¹Ÿè¦å¼•ç”¨\n",
    "                    - no: æŸ¥è©¢æ˜¯ä¸€èˆ¬æ€§å•é¡Œï¼Œå¯ä»¥ç›´æ¥å›ç­”ï¼Œä¸éœ€è¦ç‰¹å®šå°è©±å…§å®¹\n",
    "\n",
    "                    åªå…è¨±å›ç­”: yes/no\n",
    "                    \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        if \"yes\" in response:\n",
    "            return RetrieveDecision.YES\n",
    "        else:\n",
    "            return RetrieveDecision.NO\n",
    "    \n",
    "    def predict_isrel(self, query: str, dialogue: str) -> IsREL:\n",
    "        \"\"\"\n",
    "        M predicts IsREL given x, d\n",
    "        åˆ¤æ–·å°è©±æ˜¯å¦èˆ‡æŸ¥è©¢ç›¸é—œ\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                è«‹åˆ¤æ–·ä»¥ä¸‹å°è©±å…§å®¹æ˜¯å¦èˆ‡ä½¿ç”¨è€…æŸ¥è©¢ç›¸é—œï¼š\n",
    "\n",
    "                ä½¿ç”¨è€…æŸ¥è©¢: {query}\n",
    "                å°è©±å…§å®¹: {dialogue}...\n",
    "\n",
    "                åˆ¤æ–·æ¨™æº–ï¼š\n",
    "                - relevant: å°è©±åŒ…å«èˆ‡æŸ¥è©¢ç›´æ¥ç›¸é—œçš„è³‡è¨Šã€æƒ…å¢ƒæˆ–ä¸»é¡Œ\n",
    "                - irrelevant: å°è©±èˆ‡æŸ¥è©¢ç„¡é—œæˆ–é—œè¯æ€§æ¥µä½\n",
    "\n",
    "                åªå…è¨±å›ç­”: relevant/irrelevant\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        return IsREL.RELEVANT if \"relevant\" in response else IsREL.IRRELEVANT\n",
    "    \n",
    "    def predict_issup(self, query: str, dialogue: str, candidate_answer: str) -> IsSUP:\n",
    "        \"\"\"\n",
    "        M predicts IsSUP given x, y_t, d\n",
    "        åˆ¤æ–·å°è©±æ˜¯å¦æ”¯æ’å€™é¸ç­”æ¡ˆ\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    è«‹åˆ¤æ–·ä»¥ä¸‹å°è©±å…§å®¹æ˜¯å¦æ”¯æ’å€™é¸ç­”æ¡ˆä¸­çš„é™³è¿°ï¼š\n",
    "\n",
    "                    ä½¿ç”¨è€…æŸ¥è©¢: {query}\n",
    "                    å€™é¸ç­”æ¡ˆ: {candidate_answer}\n",
    "                    å°è©±å…§å®¹: {dialogue}...\n",
    "\n",
    "                    åˆ¤æ–·æ¨™æº–ï¼š\n",
    "                    - fully supported: ç­”æ¡ˆä¸­çš„é™³è¿°å®Œå…¨å¯åœ¨å°è©±ä¸­æ‰¾åˆ°ä¾æ“š\n",
    "                    - partially supported: éƒ¨åˆ†é™³è¿°æœ‰ä¾æ“šï¼Œéƒ¨åˆ†æ²’æœ‰\n",
    "                    - no support: ç­”æ¡ˆæ²’æœ‰å°è©±ä¾æ“š\n",
    "\n",
    "                    åªå…è¨±å›ç­”: fully supported/partially supported/no support\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        if \"fully supported\" in response:\n",
    "            return IsSUP.FULLY_SUPPORTED\n",
    "        elif \"partially supported\" in response:\n",
    "            return IsSUP.PARTIALLY_SUPPORTED\n",
    "        else:\n",
    "            return IsSUP.NO_SUPPORT\n",
    "    \n",
    "    def predict_isuse(self, query: str, candidate_answer: str, dialogue: str = \"\") -> IsUSE:\n",
    "        \"\"\"\n",
    "        M predicts IsUSE given x, y_t, d\n",
    "        è©•ä¼°å€™é¸ç­”æ¡ˆçš„æœ‰ç”¨æ€§\n",
    "        \"\"\"\n",
    "        context = f\"åƒè€ƒå°è©±: {dialogue}...\" if dialogue else \"\"\n",
    "        prompt = f\"\"\"\n",
    "                    è«‹è©•ä¼°ä»¥ä¸‹å€™é¸ç­”æ¡ˆå°ä½¿ç”¨è€…æŸ¥è©¢çš„æœ‰ç”¨æ€§(1-5åˆ†):\n",
    "\n",
    "                    ä½¿ç”¨è€…æŸ¥è©¢: {query}\n",
    "                    å€™é¸ç­”æ¡ˆ: {candidate_answer}\n",
    "                    {context}\n",
    "\n",
    "                    è©•åˆ†æ¨™æº–ï¼š\n",
    "                    5åˆ† - éå¸¸æœ‰ç”¨ï¼šå®Œæ•´å›ç­”å•é¡Œï¼Œæä¾›å…·é«”ç›¸é—œè³‡è¨Š\n",
    "                    4åˆ† - æœ‰ç”¨ï¼šå›ç­”ç›¸é—œä¸”æœ‰å¹«åŠ©\n",
    "                    3åˆ† - ä¸­ç­‰ï¼šéƒ¨åˆ†ç›¸é—œä½†ä¸å¤ è©³ç´°\n",
    "                    2åˆ† - è¼ƒå°‘ç”¨ï¼šç›¸é—œæ€§ä½æˆ–å¹«åŠ©æœ‰é™\n",
    "                    1åˆ† - ç„¡ç”¨ï¼šä¸ç›¸é—œæˆ–èª¤å°æ€§è³‡è¨Š\n",
    "\n",
    "                    åªå…è¨±å›ç­”æ•¸å­—: 1-5\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).strip()\n",
    "        try:\n",
    "            score = int(response)\n",
    "            return IsUSE(score) if 1 <= score <= 5 else IsUSE.MODERATELY_USEFUL\n",
    "        except:\n",
    "            return IsUSE.MODERATELY_USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf23d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueRAGSystem:\n",
    "    \"\"\"åŸºæ–¼ SAMSum å°è©±è³‡æ–™é›†çš„ RAG ç³»çµ±\"\"\"\n",
    "\n",
    "    def __init__(self, config: ConfigurationManager):\n",
    "        self.config = config\n",
    "        self.embedding_model_config = self.config.get_data_embedding_config()\n",
    "        self.embedding_model = EmbeddingModel(config= self.embedding_model_config)\n",
    "        \n",
    "        self.vector_storage_config = self.config.get_vector_storage_config()\n",
    "        self.gemini_config = self.config.get_gemini_config()\n",
    "        self.rag_sys_config = self.config.get_rag_sys_config()\n",
    "\n",
    "        self.vector_store = VectorStorage(config=self.vector_storage_config)\n",
    "        self.dataset_with_embeddings = load_from_disk(self.vector_storage_config.data_path)\n",
    "        self.gemini_client = GeminiClient(config=self.gemini_config)\n",
    "        self.config.max_retrieval_docs = self.rag_sys_config.max_retrieval_docs\n",
    "        \n",
    "        # ç³»çµ±çµ±è¨ˆ\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"retrieval_queries\": 0,\n",
    "            \"non_retrieval_queries\": 0,\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "\n",
    "        logger.info(\"Dialogue RAG System åˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    def build_knowledge_base(self):\n",
    "        \"\"\"å»ºç«‹å°è©±çŸ¥è­˜åº«\"\"\"\n",
    "        logger.info(\"é–‹å§‹å»ºç«‹å°è©±çŸ¥è­˜åº«...\")\n",
    "\n",
    "        # è¼‰å…¥å’Œè™•ç†å°è©±\n",
    "        documents = [{\"id\": item[\"id\"], \"dialogue\": item[\"dialogue\"], \"summary\": item[\"summary\"]} for item in self.dataset_with_embeddings]\n",
    "        embeddings = np.array(self.dataset_with_embeddings[\"embedding\"])\n",
    "        \n",
    "        # æ·»åŠ åˆ°å‘é‡å­˜å„²\n",
    "        self.vector_store.add_documents(documents, embeddings)\n",
    "\n",
    "    def query(self, user_query: str, conversation_history: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ä¸»è¦æŸ¥è©¢å‡½æ•¸ - å¯¦ä½œå®Œæ•´çš„å°è©±æª¢ç´¢æµç¨‹\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.stats[\"total_queries\"] += 1\n",
    "\n",
    "        # Step 1: M predicts Retrieve given (x, y_{t-1})\n",
    "        previous_generation = \"\\n\".join(conversation_history[-3:]) if conversation_history else \"\"\n",
    "        retrieve_decision = self.gemini_client.predict_retrieve(user_query, previous_generation)\n",
    "        \n",
    "\n",
    "        if retrieve_decision == RetrieveDecision.YES:\n",
    "            return self._handle_retrieval_branch(user_query, previous_generation, start_time)\n",
    "        else:\n",
    "            return self._handle_non_retrieval_branch(user_query, start_time)\n",
    "\n",
    "    def _handle_retrieval_branch(self, query: str, previous_generation: str, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"è™•ç†éœ€è¦æª¢ç´¢çš„åˆ†æ”¯\"\"\"\n",
    "        self.stats[\"retrieval_queries\"] += 1\n",
    "\n",
    "        # Step 4: æª¢ç´¢ç›¸é—œå°è©±\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            query_embedding,\n",
    "            top_k=self.rag_sys_config.max_retrieval_docs\n",
    "        )\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œçš„å°è©±å…§å®¹ä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚\",\n",
    "                \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "                \"sources\": [],\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "\n",
    "        # Step 5-7: ç‚ºæ¯å€‹ç›¸é—œå°è©±ç”Ÿæˆå€™é¸ç­”æ¡ˆä¸¦è©•ä¼°\n",
    "        candidates = []\n",
    "\n",
    "        for doc, score in retrieved_docs:\n",
    "            # åˆ¤æ–·ç›¸é—œæ€§\n",
    "            \n",
    "            relevance = self.gemini_client.predict_isrel(query, doc['dialogue'])\n",
    "            # print(f\"predict_isrel ====> {relevance}\")\n",
    "            if relevance == IsREL.RELEVANT:\n",
    "                # ç”Ÿæˆå€™é¸ç­”æ¡ˆ\n",
    "                candidate_answer = self._generate_candidate_answer(query, doc, previous_generation)\n",
    "\n",
    "                # è©•ä¼°æ”¯æ’æ€§å’Œæœ‰ç”¨æ€§\n",
    "                support_level = self.gemini_client.predict_issup(query, doc['dialogue'], candidate_answer)\n",
    "                usefulness = self.gemini_client.predict_isuse(query, candidate_answer, doc['dialogue'])\n",
    "\n",
    "                candidates.append({\n",
    "                    'answer': candidate_answer,\n",
    "                    'source_doc': doc,\n",
    "                    'is_relevant': relevance,\n",
    "                    'support_level': support_level,\n",
    "                    'usefulness_score': usefulness\n",
    "                })\n",
    "\n",
    "        if not candidates:\n",
    "            return {\n",
    "                \"answer\": \"æª¢ç´¢åˆ°çš„å°è©±å…§å®¹èˆ‡æ‚¨çš„å•é¡Œä¸å¤ªç›¸é—œï¼Œç„¡æ³•æä¾›æœ‰æ•ˆå›ç­”ã€‚\",\n",
    "                \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "                \"sources\": [doc['id'] for doc, score in retrieved_docs],\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "\n",
    "        # Step 8: é¸æ“‡æœ€ä½³å€™é¸ç­”æ¡ˆ\n",
    "        best_candidate = self._rank_candidates(candidates)\n",
    "\n",
    "        return {\n",
    "            \"answer\": best_candidate['answer'],\n",
    "            \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "            \"sources\": [best_candidate['source_doc']['id']],\n",
    "            \"relevance\": best_candidate['is_relevant'].value,\n",
    "            \"support_level\": best_candidate['support_level'].value,\n",
    "            \"usefulness_score\": best_candidate['usefulness_score'].value,\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"reference_dialogue\": best_candidate['source_doc']['dialogue'][:300] + \"...\"\n",
    "        }\n",
    "\n",
    "    def _handle_non_retrieval_branch(self, query: str, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"è™•ç†ä¸éœ€è¦æª¢ç´¢çš„åˆ†æ”¯\"\"\"\n",
    "        self.stats[\"non_retrieval_queries\"] += 1\n",
    "\n",
    "        # Step 9: ç›´æ¥ç”Ÿæˆç­”æ¡ˆ\n",
    "        generated_answer = self._generate_direct_answer(query)\n",
    "\n",
    "        # Step 10: è©•ä¼°æœ‰ç”¨æ€§\n",
    "        usefulness_score = self.gemini_client.predict_isuse(query, generated_answer)\n",
    "\n",
    "        return {\n",
    "            \"answer\": generated_answer,\n",
    "            \"retrieve_decision\": RetrieveDecision.NO.value,\n",
    "            \"sources\": [],\n",
    "            \"usefulness_score\": usefulness_score.value,\n",
    "            \"processing_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "    def _generate_candidate_answer(self, query: str, dialogue_doc: Dict[str, Any], previous_generation: str) -> str:\n",
    "        \"\"\"ç‚ºç‰¹å®šå°è©±ç”Ÿæˆå€™é¸ç­”æ¡ˆ\"\"\"\n",
    "        context = f\"è«‹åŸºæ–¼ä»¥ä¸‹å°è©±å…§å®¹å›ç­”ä½¿ç”¨è€…å•é¡Œï¼š\\n\\nå°è©±å…§å®¹: {dialogue_doc['dialogue']}\\nå°è©±æ‘˜è¦: {dialogue_doc['summary']}\"\n",
    "\n",
    "        if previous_generation:\n",
    "            context += f\"\\n\\nä¹‹å‰çš„å°è©±: {previous_generation}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "                    {context}\n",
    "\n",
    "                    ä½¿ç”¨è€…å•é¡Œ: {query}\n",
    "\n",
    "                    è«‹æä¾›æœ‰ç”¨çš„å›ç­”ï¼Œä¸¦ï¼š\n",
    "                    1. ç›´æ¥å›ç­”ä½¿ç”¨è€…å•é¡Œ\n",
    "                    2. å¼•ç”¨æˆ–æè¿°ç›¸é—œçš„å°è©±å…§å®¹\n",
    "                    3. æä¾›å…·é«”çš„è³‡è¨Šæˆ–è¦‹è§£\n",
    "                    4. ä¿æŒå›ç­”ç°¡æ½”æ˜ç¢º\n",
    "                \"\"\"\n",
    "        return self.gemini_client.generate_content(prompt)\n",
    "\n",
    "    def _generate_direct_answer(self, query: str) -> str:\n",
    "        \"\"\"ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ä½¿ç”¨æª¢ç´¢ï¼‰\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    ä½œç‚ºå°è©±ç†è§£åŠ©æ‰‹ï¼Œè«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼š\n",
    "\n",
    "                    {query}\n",
    "\n",
    "                    è«‹åŸºæ–¼ä¸€èˆ¬çŸ¥è­˜æä¾›å›ç­”ï¼Œä¸¦ï¼š\n",
    "                    1. ç›´æ¥å›ç­”å•é¡Œ\n",
    "                    2. æä¾›ç›¸é—œçš„èƒŒæ™¯è³‡è¨Š\n",
    "                    3. ä¿æŒå›ç­”æœ‰ç”¨ä¸”ç›¸é—œ\n",
    "                \"\"\"\n",
    "        return self.gemini_client.generate_content(prompt)\n",
    "\n",
    "    def _rank_candidates(self, candidates: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"æ ¹æ“š IsREL, IsSUP, IsUSE æ’åºå€™é¸ç­”æ¡ˆ\"\"\"\n",
    "        def calculate_score(candidate):\n",
    "            score = 0\n",
    "\n",
    "            # IsREL æ¬Šé‡\n",
    "            if candidate['is_relevant'] == IsREL.RELEVANT:\n",
    "                score += 10\n",
    "\n",
    "            # IsSUP æ¬Šé‡\n",
    "            support_scores = {\n",
    "                IsSUP.FULLY_SUPPORTED: 10,\n",
    "                IsSUP.PARTIALLY_SUPPORTED: 5,\n",
    "                IsSUP.NO_SUPPORT: 0\n",
    "            }\n",
    "            score += support_scores.get(candidate['support_level'], 0)\n",
    "\n",
    "            # IsUSE æ¬Šé‡\n",
    "            score += candidate['usefulness_score'].value * 2\n",
    "\n",
    "            return score\n",
    "\n",
    "        # æŒ‰åˆ†æ•¸æ’åº\n",
    "        sorted_candidates = sorted(candidates, key=calculate_score, reverse=True)\n",
    "        return sorted_candidates[0]\n",
    "\n",
    "    def get_system_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        uptime = datetime.now() - self.stats[\"start_time\"]\n",
    "        return {\n",
    "            \"total_queries\": self.stats[\"total_queries\"],\n",
    "            \"retrieval_queries\": self.stats[\"retrieval_queries\"],\n",
    "            \"non_retrieval_queries\": self.stats[\"non_retrieval_queries\"],\n",
    "            \"retrieval_rate\": self.stats[\"retrieval_queries\"] / max(1, self.stats[\"total_queries\"]),\n",
    "            \"uptime_hours\": uptime.total_seconds() / 3600,\n",
    "            \"dialogues_in_kb\": len(self.vector_store.documents)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c407632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:47:42,337: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-28 09:47:42,341: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-28 09:47:42,343: INFO: common: created directory at: artifacts]\n",
      "[2025-09-28 09:47:42,344: INFO: common: created directory at: artifacts/data_embedding]\n",
      "[2025-09-28 09:47:42,351: INFO: SentenceTransformer: Use pytorch device_name: cpu]\n",
      "[2025-09-28 09:47:42,352: INFO: SentenceTransformer: Load pretrained SentenceTransformer: ibm-granite/granite-embedding-278m-multilingual]\n",
      "[2025-09-28 09:47:48,834: INFO: 1956350838: Granite embedding model 'ibm-granite/granite-embedding-278m-multilingual' loaded successfully]\n",
      "[2025-09-28 09:47:48,835: INFO: 1956350838: VectorStore initialized with embedding_dim=768]\n",
      "[2025-09-28 09:47:49,796: INFO: 2269672936: Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: gemini-2.0-flash]\n",
      "[2025-09-28 09:47:49,797: INFO: 2765487390: Dialogue RAG System åˆå§‹åŒ–å®Œæˆ]\n",
      "[2025-09-28 09:47:49,798: INFO: 2765487390: é–‹å§‹å»ºç«‹å°è©±çŸ¥è­˜åº«...]\n",
      "[2025-09-28 09:48:10,852: INFO: 1956350838: Added 14732 documents to vector store]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    rag_system = DialogueRAGSystem(config)\n",
    "    rag_system.build_knowledge_base()\n",
    "        \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da9aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:10,892: INFO: models: AFC is enabled with max remote calls: 10.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:11,817: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: yes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:11,980: INFO: 1956350838: Encoded 1 texts into embeddings of shape (1, 768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:12,526: INFO: 1956350838: Retrieved 3 documents from vector store]\n",
      "[2025-09-28 09:48:12,537: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:13,092: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:13,097: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:16,963: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: ä»¥ä¸‹æ˜¯å›ç­”ï¼š\n",
      "\n",
      "æ‰€æœ‰åƒèˆ‡å°è©±çš„äººéƒ½è«‡è«–äº†èˆ‡å·¥ä½œç›¸é—œçš„äº‹æƒ…ã€‚ä»–å€‘åˆ†äº«äº†å„è‡ªå·¥ä½œä¸­æœ€å¥½çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼š\n",
      "\n",
      "*   **Alan** æåˆ°äº† \"training opportunities\" (åŸ¹è¨“æ©Ÿæœƒ)ã€‚\n",
      "*   **Tim** èªª \"it's the people that i work with\" (èˆ‡æˆ‘å…±äº‹çš„äºº)ã€‚\n",
      "*   **Harry** èªª \"it is challenging so i learn a lot\" (å®ƒå…·æœ‰æŒ‘æˆ°æ€§ï¼Œæ‰€ä»¥æˆ‘å­¸åˆ°å¾ˆå¤š)ã€‚\n",
      "*   **Bill** æåˆ° \"flexibility! can work different hours\" (å½ˆæ€§ï¼å¯ä»¥å·¥ä½œä¸åŒçš„æ™‚é–“)ã€‚\n",
      "*   **Jeff** èªª \"free tea & coffee, fruit and lunch once a week\" (å…è²»èŒ¶å’Œå’–å•¡ï¼Œæ°´æœå’Œæ¯é€±ä¸€æ¬¡çš„åˆé¤)ã€‚\n",
      "*   **Sarah** èªª \"that i don't have to wear posh clothes\" (æˆ‘ä¸éœ€è¦ç©¿è¯éº—çš„è¡£æœ)ã€‚\n",
      "*   **Eric** èªª \"can work from home 3 days a week so save lots of time and money on commuting\" (æ¯é€±å¯ä»¥å±…å®¶å·¥ä½œä¸‰å¤©ï¼Œæ‰€ä»¥ç¯€çœäº†å¤§é‡çš„é€šå‹¤æ™‚é–“å’Œé‡‘éŒ¢)ã€‚\n",
      "*   **Rob** æåˆ° \"my salary\" (æˆ‘çš„è–ªæ°´) å’Œ \"it is near my house so i can walk to work\" (å®ƒé›¢æˆ‘å®¶å¾ˆè¿‘ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥èµ°è·¯å»ä¸Šç­)ã€‚\n",
      "*   **Karen** èªª \"passionate people around\" (å‘¨åœæœ‰å……æ»¿ç†±æƒ…çš„äºº)ã€‚\n",
      "*   **Freddie** èªª \"great manager! i also do stuff that i care about!\" (å¾ˆæ£’çš„ç¶“ç†ï¼æˆ‘ä¹Ÿåšæˆ‘é—œå¿ƒçš„äº‹æƒ…ï¼)ã€‚\n",
      "*   **Jamie** èªª \"my favourite part of my job is that it's very creative\" (æˆ‘æœ€å–œæ­¡çš„å·¥ä½œéƒ¨åˆ†æ˜¯éå¸¸æœ‰å‰µæ„)ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œæ‰€æœ‰åƒèˆ‡è€…éƒ½åœ¨è¨è«–èˆ‡ä»–å€‘å·¥ä½œç›¸é—œçš„å„ªé»ã€‚\n",
      "\n",
      "[2025-09-28 09:48:17,035: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:17,679: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:17,685: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:18,283: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 5\n",
      "\n",
      "[2025-09-28 09:48:18,288: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:18,904: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: irrelevant\n",
      "\n",
      "[2025-09-28 09:48:18,907: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:19,703: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: John å’Œ Matthew çš„è«‡è©±èˆ‡å·¥ä½œæœ‰é—œã€‚ä»–å€‘åœ¨å°è©±ä¸­æåˆ°æ­£åœ¨ \"Writing some stupid dialogs\"ï¼Œé€™è¡¨æ˜ä»–å€‘æ­£åœ¨é€²è¡Œä¸€é …å¯«ä½œå°è©±çš„å·¥ä½œã€‚\n",
      "\n",
      "[2025-09-28 09:48:19,706: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:20,237: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:20,243: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:21,028: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 5\n",
      "\n",
      "[2025-09-28 09:48:21,031: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:21,566: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:21,569: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:23,518: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: æ‰€æœ‰åƒèˆ‡å°è©±çš„äººéƒ½è«‡è«–äº†èˆ‡å·¥ä½œç›¸é—œçš„äº‹æƒ…ã€‚\n",
      "\n",
      "*   **John:** è€ƒæ…®åœ¨å®¶å·¥ä½œï¼Œä¸¦éœ€è¦é€šçŸ¥ Simonã€‚\n",
      "*   **Mia:** é›–ç„¶åœ¨å®¶å·¥ä½œå°å¥¹ç„¡æ•ˆï¼Œä½†å¥¹ä¹Ÿåƒèˆ‡äº†é—œæ–¼ John å·¥ä½œå®‰æ’çš„è¨è«–ã€‚\n",
      "*   **Miles:** æé†’ John éœ€è¦é€šçŸ¥ Simonï¼Œå› ç‚º Simon å”èª¿è¾¦å…¬å®¤å·¥ä½œã€‚é€™è¡¨æ˜ Miles ä¹Ÿé—œå¿ƒå·¥ä½œå®‰æ’ã€‚\n",
      "\n",
      "å°è©±çš„æ ¸å¿ƒå…§å®¹æ˜¯ John çš„å·¥ä½œåœ°é»å®‰æ’ä»¥åŠå¦‚ä½•é€šçŸ¥ç›¸é—œäººå“¡ï¼Œå› æ­¤æ‰€æœ‰äººéƒ½åƒèˆ‡äº†èˆ‡å·¥ä½œç›¸é—œçš„è¨è«–ã€‚\n",
      "\n",
      "[2025-09-28 09:48:23,521: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:24,135: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:24,139: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:24,746: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 5\n",
      "\n",
      "ğŸ¤– å›ç­”: ä»¥ä¸‹æ˜¯å›ç­”ï¼š\n",
      "\n",
      "æ‰€æœ‰åƒèˆ‡å°è©±çš„äººéƒ½è«‡è«–äº†èˆ‡å·¥ä½œç›¸é—œçš„äº‹æƒ…ã€‚ä»–å€‘åˆ†äº«äº†å„è‡ªå·¥ä½œä¸­æœ€å¥½çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼š\n",
      "\n",
      "*   **Alan** æåˆ°äº† \"training opportunities\" (åŸ¹è¨“æ©Ÿæœƒ)ã€‚\n",
      "*   **Tim** èªª \"it's the people that i work with\" (èˆ‡æˆ‘å…±äº‹çš„äºº)ã€‚\n",
      "*   **Harry** èªª \"it is challenging so i learn a lot\" (å®ƒå…·æœ‰æŒ‘æˆ°æ€§ï¼Œæ‰€ä»¥æˆ‘å­¸åˆ°å¾ˆå¤š)ã€‚\n",
      "*   **Bill** æåˆ° \"flexibility! can work different hours\" (å½ˆæ€§ï¼å¯ä»¥å·¥ä½œä¸åŒçš„æ™‚é–“)ã€‚\n",
      "*   **Jeff** èªª \"free tea & coffee, fruit and lunch once a week\" (å…è²»èŒ¶å’Œå’–å•¡ï¼Œæ°´æœå’Œæ¯é€±ä¸€æ¬¡çš„åˆé¤)ã€‚\n",
      "*   **Sarah** èªª \"that i don't have to wear posh clothes\" (æˆ‘ä¸éœ€è¦ç©¿è¯éº—çš„è¡£æœ)ã€‚\n",
      "*   **Eric** èªª \"can work from home 3 days a week so save lots of time and money on commuting\" (æ¯é€±å¯ä»¥å±…å®¶å·¥ä½œä¸‰å¤©ï¼Œæ‰€ä»¥ç¯€çœäº†å¤§é‡çš„é€šå‹¤æ™‚é–“å’Œé‡‘éŒ¢)ã€‚\n",
      "*   **Rob** æåˆ° \"my salary\" (æˆ‘çš„è–ªæ°´) å’Œ \"it is near my house so i can walk to work\" (å®ƒé›¢æˆ‘å®¶å¾ˆè¿‘ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥èµ°è·¯å»ä¸Šç­)ã€‚\n",
      "*   **Karen** èªª \"passionate people around\" (å‘¨åœæœ‰å……æ»¿ç†±æƒ…çš„äºº)ã€‚\n",
      "*   **Freddie** èªª \"great manager! i also do stuff that i care about!\" (å¾ˆæ£’çš„ç¶“ç†ï¼æˆ‘ä¹Ÿåšæˆ‘é—œå¿ƒçš„äº‹æƒ…ï¼)ã€‚\n",
      "*   **Jamie** èªª \"my favourite part of my job is that it's very creative\" (æˆ‘æœ€å–œæ­¡çš„å·¥ä½œéƒ¨åˆ†æ˜¯éå¸¸æœ‰å‰µæ„)ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œæ‰€æœ‰åƒèˆ‡è€…éƒ½åœ¨è¨è«–èˆ‡ä»–å€‘å·¥ä½œç›¸é—œçš„å„ªé»ã€‚\n",
      "ğŸ“Š æª¢ç´¢æ±ºç­–: yes\n",
      "ğŸ“ åƒè€ƒä¾†æº: ['13717355-1']\n",
      "â±ï¸ è™•ç†æ™‚é–“: 13.87ç§’\n",
      "ğŸ¯ æ”¯æ’ç¨‹åº¦: fully supported\n",
      "â­ æœ‰ç”¨æ€§è©•åˆ†: 5/5\n",
      "ğŸ’¬ åƒè€ƒå°è©±: Alan: What's the best part of your job? \n",
      "Tim: it's the people that i work with\n",
      "Harry: it is challenging so i learn a lot\n",
      "Alan: yeah, training opportunities that's what i like the most\n",
      "Harry: that's great!\n",
      "Bill: flexibility! can work different hours\n",
      "Jeff: free tea & coffee, fruit and lunch once...\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ ç³»çµ±çµ±è¨ˆè³‡è¨Š\n",
      "============================================================\n",
      "total_queries: 1\n",
      "retrieval_queries: 1\n",
      "non_retrieval_queries: 0\n",
      "retrieval_rate: 1.0\n",
      "uptime_hours: 0.009708663333333334\n",
      "dialogues_in_kb: 14732\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_query = \"å“ªäº›äººè«‡è©±è·Ÿå·¥ä½œæœ‰é—œ?\"\n",
    "    result = rag_system.query(test_query)\n",
    "    \n",
    "    print(f\"ğŸ¤– å›ç­”: {result['answer']}\")\n",
    "    print(f\"ğŸ“Š æª¢ç´¢æ±ºç­–: {result['retrieve_decision']}\")\n",
    "    print(f\"ğŸ“ åƒè€ƒä¾†æº: {result.get('sources', [])}\")\n",
    "    print(f\"â±ï¸ è™•ç†æ™‚é–“: {result['processing_time']:.2f}ç§’\")\n",
    "\n",
    "    if 'support_level' in result:\n",
    "        print(f\"ğŸ¯ æ”¯æ’ç¨‹åº¦: {result['support_level']}\")\n",
    "    if 'usefulness_score' in result:\n",
    "        print(f\"â­ æœ‰ç”¨æ€§è©•åˆ†: {result['usefulness_score']}/5\")\n",
    "    if 'reference_dialogue' in result:\n",
    "        print(f\"ğŸ’¬ åƒè€ƒå°è©±: {result['reference_dialogue']}\")\n",
    "\n",
    "    # é¡¯ç¤ºç³»çµ±çµ±è¨ˆ\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ“ˆ ç³»çµ±çµ±è¨ˆè³‡è¨Š\")\n",
    "    print(f\"{'='*60}\")\n",
    "    stats = rag_system.get_system_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ff50b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»¥ä¸‹æ˜¯å›ç­”ï¼š\\n\\næ‰€æœ‰åƒèˆ‡å°è©±çš„äººéƒ½è«‡è«–äº†èˆ‡å·¥ä½œç›¸é—œçš„äº‹æƒ…ã€‚ä»–å€‘åˆ†äº«äº†å„è‡ªå·¥ä½œä¸­æœ€å¥½çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼š\\n\\n*   **Alan** æåˆ°äº† \"training opportunities\" (åŸ¹è¨“æ©Ÿæœƒ)ã€‚\\n*   **Tim** èªª \"it\\'s the people that i work with\" (èˆ‡æˆ‘å…±äº‹çš„äºº)ã€‚\\n*   **Harry** èªª \"it is challenging so i learn a lot\" (å®ƒå…·æœ‰æŒ‘æˆ°æ€§ï¼Œæ‰€ä»¥æˆ‘å­¸åˆ°å¾ˆå¤š)ã€‚\\n*   **Bill** æåˆ° \"flexibility! can work different hours\" (å½ˆæ€§ï¼å¯ä»¥å·¥ä½œä¸åŒçš„æ™‚é–“)ã€‚\\n*   **Jeff** èªª \"free tea & coffee, fruit and lunch once a week\" (å…è²»èŒ¶å’Œå’–å•¡ï¼Œæ°´æœå’Œæ¯é€±ä¸€æ¬¡çš„åˆé¤)ã€‚\\n*   **Sarah** èªª \"that i don\\'t have to wear posh clothes\" (æˆ‘ä¸éœ€è¦ç©¿è¯éº—çš„è¡£æœ)ã€‚\\n*   **Eric** èªª \"can work from home 3 days a week so save lots of time and money on commuting\" (æ¯é€±å¯ä»¥å±…å®¶å·¥ä½œä¸‰å¤©ï¼Œæ‰€ä»¥ç¯€çœäº†å¤§é‡çš„é€šå‹¤æ™‚é–“å’Œé‡‘éŒ¢)ã€‚\\n*   **Rob** æåˆ° \"my salary\" (æˆ‘çš„è–ªæ°´) å’Œ \"it is near my house so i can walk to work\" (å®ƒé›¢æˆ‘å®¶å¾ˆè¿‘ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥èµ°è·¯å»ä¸Šç­)ã€‚\\n*   **Karen** èªª \"passionate people around\" (å‘¨åœæœ‰å……æ»¿ç†±æƒ…çš„äºº)ã€‚\\n*   **Freddie** èªª \"great manager! i also do stuff that i care about!\" (å¾ˆæ£’çš„ç¶“ç†ï¼æˆ‘ä¹Ÿåšæˆ‘é—œå¿ƒçš„äº‹æƒ…ï¼)ã€‚\\n*   **Jamie** èªª \"my favourite part of my job is that it\\'s very creative\" (æˆ‘æœ€å–œæ­¡çš„å·¥ä½œéƒ¨åˆ†æ˜¯éå¸¸æœ‰å‰µæ„)ã€‚\\n\\nå› æ­¤ï¼Œæ‰€æœ‰åƒèˆ‡è€…éƒ½åœ¨è¨è«–èˆ‡ä»–å€‘å·¥ä½œç›¸é—œçš„å„ªé»ã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed11504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:24,807: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:25,521: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: yes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:25,628: INFO: 1956350838: Encoded 1 texts into embeddings of shape (1, 768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:26,162: INFO: 1956350838: Retrieved 3 documents from vector store]\n",
      "[2025-09-28 09:48:26,171: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:26,763: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:26,766: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:27,885: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: Rob å’Œ Jeffery æåˆ°äº†èˆ‡ 'salary' ç›¸é—œçš„äº‹æƒ…ã€‚\n",
      "\n",
      "*   **Rob** æåˆ° \"my salary\" ä½œç‚ºä»–å·¥ä½œä¸­æœ€å–œæ­¡çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚\n",
      "*   **Jeffery** åœ¨å¾ŒçºŒå°è©±ä¸­æåˆ°ä»– \"got my salary raised from this month!\" (é€™å€‹æœˆé–‹å§‹åŠ è–ªäº†!)ï¼Œé€™è¡¨æ˜è–ªæ°´æ˜¯ä»–é—œæ³¨çš„é‡é»ã€‚\n",
      "\n",
      "[2025-09-28 09:48:27,890: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:28,450: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: partially supported\n",
      "\n",
      "[2025-09-28 09:48:28,454: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:29,115: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 5\n",
      "\n",
      "[2025-09-28 09:48:29,118: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:29,667: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:29,671: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:30,612: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: Rob æåˆ°äº†èˆ‡è–ªæ°´ç›¸é—œçš„äº‹æƒ…ã€‚ä»–èªªä»–å·¥ä½œä¸­æœ€å¥½çš„éƒ¨åˆ†æ˜¯ \"my salary\" (æˆ‘çš„è–ªæ°´)ã€‚\n",
      "\n",
      "[2025-09-28 09:48:30,615: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:31,151: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: no support\n",
      "\n",
      "[2025-09-28 09:48:31,155: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:31,685: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 4\n",
      "\n",
      "[2025-09-28 09:48:31,688: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:32,392: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:32,395: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:33,301: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: Rob æåˆ°äº†èˆ‡ \"salary\" (è–ªæ°´) ç›¸é—œçš„äº‹æƒ…ã€‚æ ¹æ“šä¹‹å‰çš„å°è©±æ‘˜è¦ï¼ŒRob èªª \"my salary\" æ˜¯ä»–å·¥ä½œä¸­æœ€å¥½çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚\n",
      "\n",
      "[2025-09-28 09:48:33,305: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:33,832: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: no support\n",
      "\n",
      "[2025-09-28 09:48:33,835: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:34,587: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "ç”¢ç”Ÿå…§å®¹===>: 1\n",
      "\n",
      "ğŸ¤– å›ç­”: Rob å’Œ Jeffery æåˆ°äº†èˆ‡ 'salary' ç›¸é—œçš„äº‹æƒ…ã€‚\n",
      "\n",
      "*   **Rob** æåˆ° \"my salary\" ä½œç‚ºä»–å·¥ä½œä¸­æœ€å–œæ­¡çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚\n",
      "*   **Jeffery** åœ¨å¾ŒçºŒå°è©±ä¸­æåˆ°ä»– \"got my salary raised from this month!\" (é€™å€‹æœˆé–‹å§‹åŠ è–ªäº†!)ï¼Œé€™è¡¨æ˜è–ªæ°´æ˜¯ä»–é—œæ³¨çš„é‡é»ã€‚\n",
      "ğŸ“Š æª¢ç´¢æ±ºç­–: yes\n",
      "ğŸ“ åƒè€ƒä¾†æº: ['13862334']\n",
      "â±ï¸ è™•ç†æ™‚é–“: 9.78ç§’\n",
      "ğŸ¯ æ”¯æ’ç¨‹åº¦: partially supported\n",
      "â­ æœ‰ç”¨æ€§è©•åˆ†: 5/5\n",
      "ğŸ’¬ åƒè€ƒå°è©±: Jeffery: I got my salary raised from this month!(^O^)ï¼(^O^)ï¼\n",
      "Faris: HOOOOOORAAAAYYY!!! congratulations! @>â€‘â€‘>â€‘â€‘@>â€‘â€‘>â€‘â€‘@>â€‘â€‘>â€‘â€‘@>â€‘â€‘>â€‘â€‘\n",
      "Faris: I know you would! m9(^Ğ”^)m9(^Ğ”^)\n",
      "Jeffery: Thank you honey!!!!! Letâ€™s throw a big party!â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸\n",
      "Faris: I will buy some cake then11111 (*^3^)/~â˜†\n",
      "Faris: party...\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ ç³»çµ±çµ±è¨ˆè³‡è¨Š\n",
      "============================================================\n",
      "total_queries: 2\n",
      "retrieval_queries: 2\n",
      "non_retrieval_queries: 0\n",
      "retrieval_rate: 1.0\n",
      "uptime_hours: 0.012442724722222223\n",
      "dialogues_in_kb: 14732\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_query = \"æˆ‘æƒ³æ›´é€²ä¸€æ­¥çŸ¥é“ï¼Œèª°æåˆ°è·Ÿ'salary'æœ‰é—œ?\"\n",
    "    res = rag_system.query(test_query, [result['answer']])\n",
    "    \n",
    "    print(f\"ğŸ¤– å›ç­”: {res['answer']}\")\n",
    "    print(f\"ğŸ“Š æª¢ç´¢æ±ºç­–: {res['retrieve_decision']}\")\n",
    "    print(f\"ğŸ“ åƒè€ƒä¾†æº: {res.get('sources', [])}\")\n",
    "    print(f\"â±ï¸ è™•ç†æ™‚é–“: {res['processing_time']:.2f}ç§’\")\n",
    "\n",
    "    if 'support_level' in res:\n",
    "        print(f\"ğŸ¯ æ”¯æ’ç¨‹åº¦: {res['support_level']}\")\n",
    "    if 'usefulness_score' in res:\n",
    "        print(f\"â­ æœ‰ç”¨æ€§è©•åˆ†: {res['usefulness_score']}/5\")\n",
    "    if 'reference_dialogue' in res:\n",
    "        print(f\"ğŸ’¬ åƒè€ƒå°è©±: {res['reference_dialogue']}\")\n",
    "\n",
    "    # é¡¯ç¤ºç³»çµ±çµ±è¨ˆ\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ“ˆ ç³»çµ±çµ±è¨ˆè³‡è¨Š\")\n",
    "    print(f\"{'='*60}\")\n",
    "    stats = rag_system.get_system_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5a01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
