{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1156e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c958c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27da631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataEmbeddingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_name: str\n",
    "    text_column: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class VectorStorageConfig:\n",
    "    data_path: Path\n",
    "    embedding_dim: int\n",
    "\n",
    "@dataclass\n",
    "class GeminiConfig:\n",
    "    # Gemini API 設定\n",
    "    gemini_api_key: str\n",
    "    gemini_model: str\n",
    "    gemini_temperature: float\n",
    "    gemini_max_output_tokens: int\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.gemini_api_key:\n",
    "            raise ValueError(\"請設定 GEMINI_API_KEY 環境變數或在配置中提供 API key\")\n",
    "        \n",
    "@dataclass\n",
    "class RAGSystemConfig:\n",
    "    max_retrieval_docs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c86fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialogue_rag_chatbot.constants import *\n",
    "from dialogue_rag_chatbot.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_embedding_config(self) -> DataEmbeddingConfig:\n",
    "        config = self.config.data_embedding\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        data_embedding_config = DataEmbeddingConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_name = config.model_name,\n",
    "            text_column= config.text_column\n",
    "        )\n",
    "\n",
    "        return data_embedding_config\n",
    "\n",
    "    def get_vector_storage_config(self)-> VectorStorageConfig:\n",
    "\n",
    "        config = self.config.vector_storage\n",
    "        vector_storage_config = VectorStorageConfig(\n",
    "            data_path=config.data_path,\n",
    "            embedding_dim=config.embedding_dim\n",
    "        )\n",
    "\n",
    "        return vector_storage_config\n",
    "    \n",
    "    def get_gemini_config(self)-> GeminiConfig:\n",
    "        config = self.config.gemini\n",
    "        gemini_config = GeminiConfig(\n",
    "            gemini_api_key=os.getenv(\"GEMINI_API_KEY\", \"\"),\n",
    "            gemini_model=config.gemini_model,\n",
    "            gemini_temperature= config.gemini_temperature,\n",
    "            gemini_max_output_tokens=config.gemini_max_output_tokens\n",
    "        )\n",
    "\n",
    "        return gemini_config\n",
    "    \n",
    "    def get_rag_sys_config(self)-> RAGSystemConfig:\n",
    "        config = self.config.RAG_system\n",
    "        rag_sys_config = RAGSystemConfig(\n",
    "            max_retrieval_docs = config.max_retrieval_docs\n",
    "        )\n",
    "\n",
    "        return rag_sys_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kun\\miniconda3\\envs\\legalrag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dialogue_rag_chatbot.logging import logger\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from enum import Enum\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01439bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveDecision(str, Enum):\n",
    "    \"\"\"檢索決策枚舉\"\"\"\n",
    "    YES = \"yes\"\n",
    "    NO = \"no\" \n",
    "    CONTINUE = \"continue\"\n",
    "\n",
    "class IsREL(str, Enum):\n",
    "    \"\"\"相關性判斷枚舉\"\"\"\n",
    "    RELEVANT = \"relevant\"\n",
    "    IRRELEVANT = \"irrelevant\"\n",
    "\n",
    "class IsSUP(str, Enum):\n",
    "    \"\"\"支撐性判斷枚舉\"\"\"\n",
    "    FULLY_SUPPORTED = \"fully supported\"\n",
    "    PARTIALLY_SUPPORTED = \"partially supported\"\n",
    "    NO_SUPPORT = \"no support\"\n",
    "\n",
    "class IsUSE(int, Enum):\n",
    "    \"\"\"有用性評分枚舉\"\"\"\n",
    "    VERY_USEFUL = 5\n",
    "    USEFUL = 4\n",
    "    MODERATELY_USEFUL = 3\n",
    "    LESS_USEFUL = 2\n",
    "    NOT_USEFUL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbfffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Granite embedding model using SentenceTransformers\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataEmbeddingConfig):\n",
    "        self.model_name = config.model_name\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            logger.info(f\"Granite embedding model '{self.model_name}' loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {self.model_name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def encode(self, texts):\n",
    "        \"\"\"Encode texts into embeddings\"\"\"\n",
    "        try:\n",
    "            # Granite embedding models return 768-dimensional vectors\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "            logger.info(f\"Encoded {len(texts)} texts into embeddings of shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encoding texts: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class VectorStorage:\n",
    "    \"\"\"Store and retrieve document embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, config: VectorStorageConfig):\n",
    "        self.config = config\n",
    "        self.embedding_dim = self.config.embedding_dim\n",
    "        self.embeddings = []\n",
    "        self.documents = []\n",
    "        logger.info(f\"VectorStore initialized with embedding_dim={self.embedding_dim}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the store\"\"\"\n",
    "        if embeddings.shape[1] != self.embedding_dim:\n",
    "            raise ValueError(f\"Embedding dimension mismatch: expected {self.embedding_dim}, got {embeddings.shape[1]}\")\n",
    "        \n",
    "        self.embeddings.extend(embeddings.tolist())\n",
    "        self.documents.extend(documents)\n",
    "        logger.info(f\"Added {len(documents)} documents to vector store\")\n",
    "    \n",
    "    def similarity_search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        \"\"\"Perform similarity search\"\"\"\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = query_embedding.flatten()\n",
    "        \n",
    "        embeddings_matrix = np.array(self.embeddings)\n",
    "        similarities = np.dot(embeddings_matrix, query_embedding.T) / (\n",
    "            np.linalg.norm(embeddings_matrix, axis=1) * np.linalg.norm(query_embedding) + 1e-10\n",
    "        )\n",
    "        \n",
    "        # top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        top_indices = np.argpartition(-similarities, top_k)[:top_k]\n",
    "        top_indices = top_indices[np.argsort(-similarities[top_indices])]\n",
    "\n",
    "        results = [(self.documents[idx], float(similarities[idx])) for idx in top_indices]\n",
    "        logger.info(f\"Retrieved {len(results)} documents from vector store\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19369ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiClient:\n",
    "    \"\"\"Gemini API 客戶端封裝\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GeminiConfig):\n",
    "        self.config = config\n",
    "        self.client = genai.Client(api_key=config.gemini_api_key)\n",
    "        logger.info(f\"Gemini 客戶端初始化完成，使用模型: {config.gemini_model}\")\n",
    "    \n",
    "    def generate_content(self, prompt: str, context: str = \"\") -> str:\n",
    "        \"\"\"生成文本內容\"\"\"\n",
    "        try:\n",
    "            full_prompt = f\"{context}\\n\\n{prompt}\" if context else prompt\n",
    "\n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.config.gemini_model,\n",
    "                contents=full_prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=self.config.gemini_temperature,\n",
    "                    max_output_tokens=self.config.gemini_max_output_tokens\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"產生內容===>: {response.candidates[0].content.parts[0].text}\")\n",
    "            \n",
    "            if response and hasattr(response, 'text') and response.text:\n",
    "                return response.text.strip()\n",
    "            else:\n",
    "                # 如果因為安全設定等原因沒有文字返回，則返回空字串\n",
    "                logger.warning(f\"Gemini API did not return text. Full response: {response}\")\n",
    "                return \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini API 呼叫失敗: {str(e)}\")\n",
    "            return \"抱歉，系統暫時無法處理您的請求。\"\n",
    "    \n",
    "    def predict_retrieve(self, query: str, previous_generation: str = \"\") -> RetrieveDecision:\n",
    "        \"\"\"\n",
    "        M predicts Retrieve given (x, y_{t-1})\n",
    "        判斷是否需要檢索對話資料來回答問題\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    請判斷以下使用者查詢是否需要檢索對話資料庫來回答：\n",
    "\n",
    "                    目前使用者查詢: {query}\n",
    "                    之前的對話內容: {previous_generation if previous_generation else \"無\"}\n",
    "\n",
    "                    判斷標準：\n",
    "                    - yes: 查詢需要具體的對話內容或對話場景來回答。如果有之前的對話內容，請務必也要引用\n",
    "                    - no: 查詢是一般性問題，可以直接回答，不需要特定對話內容\n",
    "\n",
    "                    只允許回答: yes/no\n",
    "                    \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        if \"yes\" in response:\n",
    "            return RetrieveDecision.YES\n",
    "        else:\n",
    "            return RetrieveDecision.NO\n",
    "    \n",
    "    def predict_isrel(self, query: str, dialogue: str) -> IsREL:\n",
    "        \"\"\"\n",
    "        M predicts IsREL given x, d\n",
    "        判斷對話是否與查詢相關\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                請判斷以下對話內容是否與使用者查詢相關：\n",
    "\n",
    "                使用者查詢: {query}\n",
    "                對話內容: {dialogue}...\n",
    "\n",
    "                判斷標準：\n",
    "                - relevant: 對話包含與查詢直接相關的資訊、情境或主題\n",
    "                - irrelevant: 對話與查詢無關或關聯性極低\n",
    "\n",
    "                只允許回答: relevant/irrelevant\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        return IsREL.RELEVANT if \"relevant\" in response else IsREL.IRRELEVANT\n",
    "    \n",
    "    def predict_issup(self, query: str, dialogue: str, candidate_answer: str) -> IsSUP:\n",
    "        \"\"\"\n",
    "        M predicts IsSUP given x, y_t, d\n",
    "        判斷對話是否支撐候選答案\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    請判斷以下對話內容是否支撐候選答案中的陳述：\n",
    "\n",
    "                    使用者查詢: {query}\n",
    "                    候選答案: {candidate_answer}\n",
    "                    對話內容: {dialogue}...\n",
    "\n",
    "                    判斷標準：\n",
    "                    - fully supported: 答案中的陳述完全可在對話中找到依據\n",
    "                    - partially supported: 部分陳述有依據，部分沒有\n",
    "                    - no support: 答案沒有對話依據\n",
    "\n",
    "                    只允許回答: fully supported/partially supported/no support\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).lower().strip()\n",
    "        if \"fully supported\" in response:\n",
    "            return IsSUP.FULLY_SUPPORTED\n",
    "        elif \"partially supported\" in response:\n",
    "            return IsSUP.PARTIALLY_SUPPORTED\n",
    "        else:\n",
    "            return IsSUP.NO_SUPPORT\n",
    "    \n",
    "    def predict_isuse(self, query: str, candidate_answer: str, dialogue: str = \"\") -> IsUSE:\n",
    "        \"\"\"\n",
    "        M predicts IsUSE given x, y_t, d\n",
    "        評估候選答案的有用性\n",
    "        \"\"\"\n",
    "        context = f\"參考對話: {dialogue}...\" if dialogue else \"\"\n",
    "        prompt = f\"\"\"\n",
    "                    請評估以下候選答案對使用者查詢的有用性(1-5分):\n",
    "\n",
    "                    使用者查詢: {query}\n",
    "                    候選答案: {candidate_answer}\n",
    "                    {context}\n",
    "\n",
    "                    評分標準：\n",
    "                    5分 - 非常有用：完整回答問題，提供具體相關資訊\n",
    "                    4分 - 有用：回答相關且有幫助\n",
    "                    3分 - 中等：部分相關但不夠詳細\n",
    "                    2分 - 較少用：相關性低或幫助有限\n",
    "                    1分 - 無用：不相關或誤導性資訊\n",
    "\n",
    "                    只允許回答數字: 1-5\n",
    "                \"\"\"\n",
    "        response = self.generate_content(prompt).strip()\n",
    "        try:\n",
    "            score = int(response)\n",
    "            return IsUSE(score) if 1 <= score <= 5 else IsUSE.MODERATELY_USEFUL\n",
    "        except:\n",
    "            return IsUSE.MODERATELY_USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf23d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueRAGSystem:\n",
    "    \"\"\"基於 SAMSum 對話資料集的 RAG 系統\"\"\"\n",
    "\n",
    "    def __init__(self, config: ConfigurationManager):\n",
    "        self.config = config\n",
    "        self.embedding_model_config = self.config.get_data_embedding_config()\n",
    "        self.embedding_model = EmbeddingModel(config= self.embedding_model_config)\n",
    "        \n",
    "        self.vector_storage_config = self.config.get_vector_storage_config()\n",
    "        self.gemini_config = self.config.get_gemini_config()\n",
    "        self.rag_sys_config = self.config.get_rag_sys_config()\n",
    "\n",
    "        self.vector_store = VectorStorage(config=self.vector_storage_config)\n",
    "        self.dataset_with_embeddings = load_from_disk(self.vector_storage_config.data_path)\n",
    "        self.gemini_client = GeminiClient(config=self.gemini_config)\n",
    "        self.config.max_retrieval_docs = self.rag_sys_config.max_retrieval_docs\n",
    "        \n",
    "        # 系統統計\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"retrieval_queries\": 0,\n",
    "            \"non_retrieval_queries\": 0,\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "\n",
    "        logger.info(\"Dialogue RAG System 初始化完成\")\n",
    "\n",
    "    def build_knowledge_base(self):\n",
    "        \"\"\"建立對話知識庫\"\"\"\n",
    "        logger.info(\"開始建立對話知識庫...\")\n",
    "\n",
    "        # 載入和處理對話\n",
    "        documents = [{\"id\": item[\"id\"], \"dialogue\": item[\"dialogue\"], \"summary\": item[\"summary\"]} for item in self.dataset_with_embeddings]\n",
    "        embeddings = np.array(self.dataset_with_embeddings[\"embedding\"])\n",
    "        \n",
    "        # 添加到向量存儲\n",
    "        self.vector_store.add_documents(documents, embeddings)\n",
    "\n",
    "    def query(self, user_query: str, conversation_history: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        主要查詢函數 - 實作完整的對話檢索流程\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.stats[\"total_queries\"] += 1\n",
    "\n",
    "        # Step 1: M predicts Retrieve given (x, y_{t-1})\n",
    "        previous_generation = \"\\n\".join(conversation_history[-3:]) if conversation_history else \"\"\n",
    "        retrieve_decision = self.gemini_client.predict_retrieve(user_query, previous_generation)\n",
    "        \n",
    "\n",
    "        if retrieve_decision == RetrieveDecision.YES:\n",
    "            return self._handle_retrieval_branch(user_query, previous_generation, start_time)\n",
    "        else:\n",
    "            return self._handle_non_retrieval_branch(user_query, start_time)\n",
    "\n",
    "    def _handle_retrieval_branch(self, query: str, previous_generation: str, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"處理需要檢索的分支\"\"\"\n",
    "        self.stats[\"retrieval_queries\"] += 1\n",
    "\n",
    "        # Step 4: 檢索相關對話\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        retrieved_docs = self.vector_store.similarity_search(\n",
    "            query_embedding,\n",
    "            top_k=self.rag_sys_config.max_retrieval_docs\n",
    "        )\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"抱歉，我找不到相關的對話內容來回答您的問題。\",\n",
    "                \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "                \"sources\": [],\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "\n",
    "        # Step 5-7: 為每個相關對話生成候選答案並評估\n",
    "        candidates = []\n",
    "\n",
    "        for doc, score in retrieved_docs:\n",
    "            # 判斷相關性\n",
    "            \n",
    "            relevance = self.gemini_client.predict_isrel(query, doc['dialogue'])\n",
    "            # print(f\"predict_isrel ====> {relevance}\")\n",
    "            if relevance == IsREL.RELEVANT:\n",
    "                # 生成候選答案\n",
    "                candidate_answer = self._generate_candidate_answer(query, doc, previous_generation)\n",
    "\n",
    "                # 評估支撐性和有用性\n",
    "                support_level = self.gemini_client.predict_issup(query, doc['dialogue'], candidate_answer)\n",
    "                usefulness = self.gemini_client.predict_isuse(query, candidate_answer, doc['dialogue'])\n",
    "\n",
    "                candidates.append({\n",
    "                    'answer': candidate_answer,\n",
    "                    'source_doc': doc,\n",
    "                    'is_relevant': relevance,\n",
    "                    'support_level': support_level,\n",
    "                    'usefulness_score': usefulness\n",
    "                })\n",
    "\n",
    "        if not candidates:\n",
    "            return {\n",
    "                \"answer\": \"檢索到的對話內容與您的問題不太相關，無法提供有效回答。\",\n",
    "                \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "                \"sources\": [doc['id'] for doc, score in retrieved_docs],\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "\n",
    "        # Step 8: 選擇最佳候選答案\n",
    "        best_candidate = self._rank_candidates(candidates)\n",
    "\n",
    "        return {\n",
    "            \"answer\": best_candidate['answer'],\n",
    "            \"retrieve_decision\": RetrieveDecision.YES.value,\n",
    "            \"sources\": [best_candidate['source_doc']['id']],\n",
    "            \"relevance\": best_candidate['is_relevant'].value,\n",
    "            \"support_level\": best_candidate['support_level'].value,\n",
    "            \"usefulness_score\": best_candidate['usefulness_score'].value,\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"reference_dialogue\": best_candidate['source_doc']['dialogue'][:300] + \"...\"\n",
    "        }\n",
    "\n",
    "    def _handle_non_retrieval_branch(self, query: str, start_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"處理不需要檢索的分支\"\"\"\n",
    "        self.stats[\"non_retrieval_queries\"] += 1\n",
    "\n",
    "        # Step 9: 直接生成答案\n",
    "        generated_answer = self._generate_direct_answer(query)\n",
    "\n",
    "        # Step 10: 評估有用性\n",
    "        usefulness_score = self.gemini_client.predict_isuse(query, generated_answer)\n",
    "\n",
    "        return {\n",
    "            \"answer\": generated_answer,\n",
    "            \"retrieve_decision\": RetrieveDecision.NO.value,\n",
    "            \"sources\": [],\n",
    "            \"usefulness_score\": usefulness_score.value,\n",
    "            \"processing_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "    def _generate_candidate_answer(self, query: str, dialogue_doc: Dict[str, Any], previous_generation: str) -> str:\n",
    "        \"\"\"為特定對話生成候選答案\"\"\"\n",
    "        context = f\"請基於以下對話內容回答使用者問題：\\n\\n對話內容: {dialogue_doc['dialogue']}\\n對話摘要: {dialogue_doc['summary']}\"\n",
    "\n",
    "        if previous_generation:\n",
    "            context += f\"\\n\\n之前的對話: {previous_generation}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "                    {context}\n",
    "\n",
    "                    使用者問題: {query}\n",
    "\n",
    "                    請提供有用的回答，並：\n",
    "                    1. 直接回答使用者問題\n",
    "                    2. 引用或描述相關的對話內容\n",
    "                    3. 提供具體的資訊或見解\n",
    "                    4. 保持回答簡潔明確\n",
    "                \"\"\"\n",
    "        return self.gemini_client.generate_content(prompt)\n",
    "\n",
    "    def _generate_direct_answer(self, query: str) -> str:\n",
    "        \"\"\"直接生成答案（不使用檢索）\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "                    作為對話理解助手，請回答以下問題：\n",
    "\n",
    "                    {query}\n",
    "\n",
    "                    請基於一般知識提供回答，並：\n",
    "                    1. 直接回答問題\n",
    "                    2. 提供相關的背景資訊\n",
    "                    3. 保持回答有用且相關\n",
    "                \"\"\"\n",
    "        return self.gemini_client.generate_content(prompt)\n",
    "\n",
    "    def _rank_candidates(self, candidates: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"根據 IsREL, IsSUP, IsUSE 排序候選答案\"\"\"\n",
    "        def calculate_score(candidate):\n",
    "            score = 0\n",
    "\n",
    "            # IsREL 權重\n",
    "            if candidate['is_relevant'] == IsREL.RELEVANT:\n",
    "                score += 10\n",
    "\n",
    "            # IsSUP 權重\n",
    "            support_scores = {\n",
    "                IsSUP.FULLY_SUPPORTED: 10,\n",
    "                IsSUP.PARTIALLY_SUPPORTED: 5,\n",
    "                IsSUP.NO_SUPPORT: 0\n",
    "            }\n",
    "            score += support_scores.get(candidate['support_level'], 0)\n",
    "\n",
    "            # IsUSE 權重\n",
    "            score += candidate['usefulness_score'].value * 2\n",
    "\n",
    "            return score\n",
    "\n",
    "        # 按分數排序\n",
    "        sorted_candidates = sorted(candidates, key=calculate_score, reverse=True)\n",
    "        return sorted_candidates[0]\n",
    "\n",
    "    def get_system_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"獲取系統統計資訊\"\"\"\n",
    "        uptime = datetime.now() - self.stats[\"start_time\"]\n",
    "        return {\n",
    "            \"total_queries\": self.stats[\"total_queries\"],\n",
    "            \"retrieval_queries\": self.stats[\"retrieval_queries\"],\n",
    "            \"non_retrieval_queries\": self.stats[\"non_retrieval_queries\"],\n",
    "            \"retrieval_rate\": self.stats[\"retrieval_queries\"] / max(1, self.stats[\"total_queries\"]),\n",
    "            \"uptime_hours\": uptime.total_seconds() / 3600,\n",
    "            \"dialogues_in_kb\": len(self.vector_store.documents)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c407632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:47:42,337: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-28 09:47:42,341: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-28 09:47:42,343: INFO: common: created directory at: artifacts]\n",
      "[2025-09-28 09:47:42,344: INFO: common: created directory at: artifacts/data_embedding]\n",
      "[2025-09-28 09:47:42,351: INFO: SentenceTransformer: Use pytorch device_name: cpu]\n",
      "[2025-09-28 09:47:42,352: INFO: SentenceTransformer: Load pretrained SentenceTransformer: ibm-granite/granite-embedding-278m-multilingual]\n",
      "[2025-09-28 09:47:48,834: INFO: 1956350838: Granite embedding model 'ibm-granite/granite-embedding-278m-multilingual' loaded successfully]\n",
      "[2025-09-28 09:47:48,835: INFO: 1956350838: VectorStore initialized with embedding_dim=768]\n",
      "[2025-09-28 09:47:49,796: INFO: 2269672936: Gemini 客戶端初始化完成，使用模型: gemini-2.0-flash]\n",
      "[2025-09-28 09:47:49,797: INFO: 2765487390: Dialogue RAG System 初始化完成]\n",
      "[2025-09-28 09:47:49,798: INFO: 2765487390: 開始建立對話知識庫...]\n",
      "[2025-09-28 09:48:10,852: INFO: 1956350838: Added 14732 documents to vector store]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    rag_system = DialogueRAGSystem(config)\n",
    "    rag_system.build_knowledge_base()\n",
    "        \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da9aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:10,892: INFO: models: AFC is enabled with max remote calls: 10.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:11,817: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: yes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:11,980: INFO: 1956350838: Encoded 1 texts into embeddings of shape (1, 768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:12,526: INFO: 1956350838: Retrieved 3 documents from vector store]\n",
      "[2025-09-28 09:48:12,537: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:13,092: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:13,097: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:16,963: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 以下是回答：\n",
      "\n",
      "所有參與對話的人都談論了與工作相關的事情。他們分享了各自工作中最好的部分。例如：\n",
      "\n",
      "*   **Alan** 提到了 \"training opportunities\" (培訓機會)。\n",
      "*   **Tim** 說 \"it's the people that i work with\" (與我共事的人)。\n",
      "*   **Harry** 說 \"it is challenging so i learn a lot\" (它具有挑戰性，所以我學到很多)。\n",
      "*   **Bill** 提到 \"flexibility! can work different hours\" (彈性！可以工作不同的時間)。\n",
      "*   **Jeff** 說 \"free tea & coffee, fruit and lunch once a week\" (免費茶和咖啡，水果和每週一次的午餐)。\n",
      "*   **Sarah** 說 \"that i don't have to wear posh clothes\" (我不需要穿華麗的衣服)。\n",
      "*   **Eric** 說 \"can work from home 3 days a week so save lots of time and money on commuting\" (每週可以居家工作三天，所以節省了大量的通勤時間和金錢)。\n",
      "*   **Rob** 提到 \"my salary\" (我的薪水) 和 \"it is near my house so i can walk to work\" (它離我家很近，所以我可以走路去上班)。\n",
      "*   **Karen** 說 \"passionate people around\" (周圍有充滿熱情的人)。\n",
      "*   **Freddie** 說 \"great manager! i also do stuff that i care about!\" (很棒的經理！我也做我關心的事情！)。\n",
      "*   **Jamie** 說 \"my favourite part of my job is that it's very creative\" (我最喜歡的工作部分是非常有創意)。\n",
      "\n",
      "因此，所有參與者都在討論與他們工作相關的優點。\n",
      "\n",
      "[2025-09-28 09:48:17,035: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:17,679: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:17,685: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:18,283: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 5\n",
      "\n",
      "[2025-09-28 09:48:18,288: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:18,904: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: irrelevant\n",
      "\n",
      "[2025-09-28 09:48:18,907: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:19,703: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: John 和 Matthew 的談話與工作有關。他們在對話中提到正在 \"Writing some stupid dialogs\"，這表明他們正在進行一項寫作對話的工作。\n",
      "\n",
      "[2025-09-28 09:48:19,706: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:20,237: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:20,243: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:21,028: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 5\n",
      "\n",
      "[2025-09-28 09:48:21,031: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:21,566: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:21,569: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:23,518: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 所有參與對話的人都談論了與工作相關的事情。\n",
      "\n",
      "*   **John:** 考慮在家工作，並需要通知 Simon。\n",
      "*   **Mia:** 雖然在家工作對她無效，但她也參與了關於 John 工作安排的討論。\n",
      "*   **Miles:** 提醒 John 需要通知 Simon，因為 Simon 協調辦公室工作。這表明 Miles 也關心工作安排。\n",
      "\n",
      "對話的核心內容是 John 的工作地點安排以及如何通知相關人員，因此所有人都參與了與工作相關的討論。\n",
      "\n",
      "[2025-09-28 09:48:23,521: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:24,135: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: fully supported\n",
      "\n",
      "[2025-09-28 09:48:24,139: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:24,746: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 5\n",
      "\n",
      "🤖 回答: 以下是回答：\n",
      "\n",
      "所有參與對話的人都談論了與工作相關的事情。他們分享了各自工作中最好的部分。例如：\n",
      "\n",
      "*   **Alan** 提到了 \"training opportunities\" (培訓機會)。\n",
      "*   **Tim** 說 \"it's the people that i work with\" (與我共事的人)。\n",
      "*   **Harry** 說 \"it is challenging so i learn a lot\" (它具有挑戰性，所以我學到很多)。\n",
      "*   **Bill** 提到 \"flexibility! can work different hours\" (彈性！可以工作不同的時間)。\n",
      "*   **Jeff** 說 \"free tea & coffee, fruit and lunch once a week\" (免費茶和咖啡，水果和每週一次的午餐)。\n",
      "*   **Sarah** 說 \"that i don't have to wear posh clothes\" (我不需要穿華麗的衣服)。\n",
      "*   **Eric** 說 \"can work from home 3 days a week so save lots of time and money on commuting\" (每週可以居家工作三天，所以節省了大量的通勤時間和金錢)。\n",
      "*   **Rob** 提到 \"my salary\" (我的薪水) 和 \"it is near my house so i can walk to work\" (它離我家很近，所以我可以走路去上班)。\n",
      "*   **Karen** 說 \"passionate people around\" (周圍有充滿熱情的人)。\n",
      "*   **Freddie** 說 \"great manager! i also do stuff that i care about!\" (很棒的經理！我也做我關心的事情！)。\n",
      "*   **Jamie** 說 \"my favourite part of my job is that it's very creative\" (我最喜歡的工作部分是非常有創意)。\n",
      "\n",
      "因此，所有參與者都在討論與他們工作相關的優點。\n",
      "📊 檢索決策: yes\n",
      "📁 參考來源: ['13717355-1']\n",
      "⏱️ 處理時間: 13.87秒\n",
      "🎯 支撐程度: fully supported\n",
      "⭐ 有用性評分: 5/5\n",
      "💬 參考對話: Alan: What's the best part of your job? \n",
      "Tim: it's the people that i work with\n",
      "Harry: it is challenging so i learn a lot\n",
      "Alan: yeah, training opportunities that's what i like the most\n",
      "Harry: that's great!\n",
      "Bill: flexibility! can work different hours\n",
      "Jeff: free tea & coffee, fruit and lunch once...\n",
      "\n",
      "============================================================\n",
      "📈 系統統計資訊\n",
      "============================================================\n",
      "total_queries: 1\n",
      "retrieval_queries: 1\n",
      "non_retrieval_queries: 0\n",
      "retrieval_rate: 1.0\n",
      "uptime_hours: 0.009708663333333334\n",
      "dialogues_in_kb: 14732\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_query = \"哪些人談話跟工作有關?\"\n",
    "    result = rag_system.query(test_query)\n",
    "    \n",
    "    print(f\"🤖 回答: {result['answer']}\")\n",
    "    print(f\"📊 檢索決策: {result['retrieve_decision']}\")\n",
    "    print(f\"📁 參考來源: {result.get('sources', [])}\")\n",
    "    print(f\"⏱️ 處理時間: {result['processing_time']:.2f}秒\")\n",
    "\n",
    "    if 'support_level' in result:\n",
    "        print(f\"🎯 支撐程度: {result['support_level']}\")\n",
    "    if 'usefulness_score' in result:\n",
    "        print(f\"⭐ 有用性評分: {result['usefulness_score']}/5\")\n",
    "    if 'reference_dialogue' in result:\n",
    "        print(f\"💬 參考對話: {result['reference_dialogue']}\")\n",
    "\n",
    "    # 顯示系統統計\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"📈 系統統計資訊\")\n",
    "    print(f\"{'='*60}\")\n",
    "    stats = rag_system.get_system_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ff50b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下是回答：\\n\\n所有參與對話的人都談論了與工作相關的事情。他們分享了各自工作中最好的部分。例如：\\n\\n*   **Alan** 提到了 \"training opportunities\" (培訓機會)。\\n*   **Tim** 說 \"it\\'s the people that i work with\" (與我共事的人)。\\n*   **Harry** 說 \"it is challenging so i learn a lot\" (它具有挑戰性，所以我學到很多)。\\n*   **Bill** 提到 \"flexibility! can work different hours\" (彈性！可以工作不同的時間)。\\n*   **Jeff** 說 \"free tea & coffee, fruit and lunch once a week\" (免費茶和咖啡，水果和每週一次的午餐)。\\n*   **Sarah** 說 \"that i don\\'t have to wear posh clothes\" (我不需要穿華麗的衣服)。\\n*   **Eric** 說 \"can work from home 3 days a week so save lots of time and money on commuting\" (每週可以居家工作三天，所以節省了大量的通勤時間和金錢)。\\n*   **Rob** 提到 \"my salary\" (我的薪水) 和 \"it is near my house so i can walk to work\" (它離我家很近，所以我可以走路去上班)。\\n*   **Karen** 說 \"passionate people around\" (周圍有充滿熱情的人)。\\n*   **Freddie** 說 \"great manager! i also do stuff that i care about!\" (很棒的經理！我也做我關心的事情！)。\\n*   **Jamie** 說 \"my favourite part of my job is that it\\'s very creative\" (我最喜歡的工作部分是非常有創意)。\\n\\n因此，所有參與者都在討論與他們工作相關的優點。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed11504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:24,807: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:25,521: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: yes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:25,628: INFO: 1956350838: Encoded 1 texts into embeddings of shape (1, 768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-28 09:48:26,162: INFO: 1956350838: Retrieved 3 documents from vector store]\n",
      "[2025-09-28 09:48:26,171: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:26,763: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:26,766: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:27,885: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: Rob 和 Jeffery 提到了與 'salary' 相關的事情。\n",
      "\n",
      "*   **Rob** 提到 \"my salary\" 作為他工作中最喜歡的部分之一。\n",
      "*   **Jeffery** 在後續對話中提到他 \"got my salary raised from this month!\" (這個月開始加薪了!)，這表明薪水是他關注的重點。\n",
      "\n",
      "[2025-09-28 09:48:27,890: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:28,450: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: partially supported\n",
      "\n",
      "[2025-09-28 09:48:28,454: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:29,115: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 5\n",
      "\n",
      "[2025-09-28 09:48:29,118: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:29,667: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:29,671: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:30,612: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: Rob 提到了與薪水相關的事情。他說他工作中最好的部分是 \"my salary\" (我的薪水)。\n",
      "\n",
      "[2025-09-28 09:48:30,615: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:31,151: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: no support\n",
      "\n",
      "[2025-09-28 09:48:31,155: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:31,685: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 4\n",
      "\n",
      "[2025-09-28 09:48:31,688: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:32,392: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: relevant\n",
      "\n",
      "[2025-09-28 09:48:32,395: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:33,301: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: Rob 提到了與 \"salary\" (薪水) 相關的事情。根據之前的對話摘要，Rob 說 \"my salary\" 是他工作中最好的部分之一。\n",
      "\n",
      "[2025-09-28 09:48:33,305: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:33,832: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: no support\n",
      "\n",
      "[2025-09-28 09:48:33,835: INFO: models: AFC is enabled with max remote calls: 10.]\n",
      "[2025-09-28 09:48:34,587: INFO: _client: HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"]\n",
      "產生內容===>: 1\n",
      "\n",
      "🤖 回答: Rob 和 Jeffery 提到了與 'salary' 相關的事情。\n",
      "\n",
      "*   **Rob** 提到 \"my salary\" 作為他工作中最喜歡的部分之一。\n",
      "*   **Jeffery** 在後續對話中提到他 \"got my salary raised from this month!\" (這個月開始加薪了!)，這表明薪水是他關注的重點。\n",
      "📊 檢索決策: yes\n",
      "📁 參考來源: ['13862334']\n",
      "⏱️ 處理時間: 9.78秒\n",
      "🎯 支撐程度: partially supported\n",
      "⭐ 有用性評分: 5/5\n",
      "💬 參考對話: Jeffery: I got my salary raised from this month!(^O^)／(^O^)／\n",
      "Faris: HOOOOOORAAAAYYY!!! congratulations! @>‑‑>‑‑@>‑‑>‑‑@>‑‑>‑‑@>‑‑>‑‑\n",
      "Faris: I know you would! m9(^Д^)m9(^Д^)\n",
      "Jeffery: Thank you honey!!!!! Let’s throw a big party!❤️❤️❤️❤️❤️❤️\n",
      "Faris: I will buy some cake then11111 (*^3^)/~☆\n",
      "Faris: party...\n",
      "\n",
      "============================================================\n",
      "📈 系統統計資訊\n",
      "============================================================\n",
      "total_queries: 2\n",
      "retrieval_queries: 2\n",
      "non_retrieval_queries: 0\n",
      "retrieval_rate: 1.0\n",
      "uptime_hours: 0.012442724722222223\n",
      "dialogues_in_kb: 14732\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_query = \"我想更進一步知道，誰提到跟'salary'有關?\"\n",
    "    res = rag_system.query(test_query, [result['answer']])\n",
    "    \n",
    "    print(f\"🤖 回答: {res['answer']}\")\n",
    "    print(f\"📊 檢索決策: {res['retrieve_decision']}\")\n",
    "    print(f\"📁 參考來源: {res.get('sources', [])}\")\n",
    "    print(f\"⏱️ 處理時間: {res['processing_time']:.2f}秒\")\n",
    "\n",
    "    if 'support_level' in res:\n",
    "        print(f\"🎯 支撐程度: {res['support_level']}\")\n",
    "    if 'usefulness_score' in res:\n",
    "        print(f\"⭐ 有用性評分: {res['usefulness_score']}/5\")\n",
    "    if 'reference_dialogue' in res:\n",
    "        print(f\"💬 參考對話: {res['reference_dialogue']}\")\n",
    "\n",
    "    # 顯示系統統計\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"📈 系統統計資訊\")\n",
    "    print(f\"{'='*60}\")\n",
    "    stats = rag_system.get_system_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5a01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
